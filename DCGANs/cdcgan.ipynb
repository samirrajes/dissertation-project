{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7c97f5-68fb-4100-ba57-5d8421637ad2",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Make sure you have the packages installed. You might need to pip install some of them, the console should tell you if you are missing any packages when you try to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c66e39-9925-4dac-8ee1-bbfa602c7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch and core machine learning libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Libraries for data loading and pre-processing\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for displaying images\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Imports for quantative metrics calculations\n",
    "from torchvision.utils import save_image\n",
    "from torch_fidelity import calculate_metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78170ac4-b5a8-4b09-ba6b-edaa064727d5",
   "metadata": {},
   "source": [
    "# Helper function for displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2c34c-7351-4dbe-88d8-3e493494b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels=None, title=\"\", num_samples=20, cols=4):\n",
    "    # ensure we don't exceed the number of available images\n",
    "    images = images[:min(num_samples, len(images))]\n",
    "\n",
    "    # create a grid of images\n",
    "    # normalize each image\n",
    "    grid = make_grid(images, nrow=cols, normalize=True, scale_each=True)  # Adjust grid columns\n",
    "    \n",
    "    # convert the grid to a PIL image\n",
    "    grid_img = to_pil_image(grid)\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 12))  # You can adjust the figure size as needed\n",
    "    plt.imshow(grid_img, cmap=\"gray\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # if labels are provided, display them (note: labels arent displayed very well)\n",
    "    if labels is not None:\n",
    "        num_images = len(images)\n",
    "        rows = (num_images + cols - 1) // cols  # Calculate the number of rows in the grid\n",
    "        for i, label in enumerate(labels[:num_images]):\n",
    "            plt.text(\n",
    "                (i % cols) * grid_img.width / cols, \n",
    "                (i // cols + 1) * grid_img.height / rows - 10,  # Adjust text position\n",
    "                label, \n",
    "                horizontalalignment='center',\n",
    "                fontsize=10,\n",
    "                color='white',\n",
    "                weight='bold'\n",
    "            )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467532e-e035-4b05-92f5-e4ed30d09c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device Type: {device} \" + (f\"| Name: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010f760-1ce2-4f8d-b4ed-410fcdc78a01",
   "metadata": {},
   "source": [
    "# Class-Conditioned Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e211c08-92ea-40cb-9d60-f50118afe433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(n_classes, 28*28)  # each label is embedded to match the image size (28x28)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # First convolutional downsampling\n",
    "            nn.Conv2d(in_channels + 1, 128, kernel_size=3, stride=2, padding=1),  # input has two channels now\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Second convolutional downsampling\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Flatenning the output for the dense layer\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            # Final classifier layer\n",
    "            nn.Linear(128 * 7 * 7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img, labels):\n",
    "        # creating label embeddings\n",
    "        label_embeddings = self.label_embedding(labels)\n",
    "        label_embeddings = label_embeddings.view(img.size(0), 1, 28, 28)  # Reshape embeddings to (B, 1, 28, 28)\n",
    "\n",
    "        # concatenating label embeddings and image\n",
    "        combined_input = torch.cat([img, label_embeddings], dim=1)  # Concatenate along the channel dimension\n",
    "\n",
    "        # forward pass through the model\n",
    "        return self.model(combined_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aad4b7-4164-448d-ad78-d5d3af06014a",
   "metadata": {},
   "source": [
    "# Class-conditioned Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb85217-5c75-40a7-8530-27c43f7d1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_classes=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # embedding and linear layer to match label dimensions to feature map dimensions\n",
    "        self.label_embedding = nn.Embedding(n_classes, 50)\n",
    "        self.fc_label = nn.Linear(50, 7 * 7)  # resize from embedding size to 7x7 feature map\n",
    "        \n",
    "        # transform the latent vector\n",
    "        self.fc_latent = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 7 * 7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(128 * 7 * 7)\n",
    "        )\n",
    "        \n",
    "        # Multiple fractionally strided convolutions to upsample from 7x7 to 28x28\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(129, 128, kernel_size=4, stride=2, padding=1),  # Note: 129 channels, concatenation of gen + label feature maps\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 1, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        # create and process label embeddings\n",
    "        labels_embedded = self.label_embedding(labels)\n",
    "        labels_transformed = self.fc_label(labels_embedded).view(-1, 1, 7, 7)\n",
    "        \n",
    "        # process the initial latent vector\n",
    "        latent_transformed = self.fc_latent(z).view(-1, 128, 7, 7)\n",
    "        \n",
    "        # concatenate label embeddings and transofmred latent vector along the channel dimension\n",
    "        combined_input = torch.cat([latent_transformed, labels_transformed], dim=1)\n",
    "        \n",
    "        # pass through the transposed convolution layers\n",
    "        return self.conv_transpose_layers(combined_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0f700-74e7-450e-ae34-ce0c20a456c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified to now also return random labels for gan training\n",
    "def generate_latent_points(latent_dim, batch_size, n_classes=10, device='cpu'):\n",
    "    # generate points in the latent space\n",
    "    z_input = torch.randn(batch_size, latent_dim, device=device)\n",
    "    # generate random labels\n",
    "    labels = torch.randint(0, n_classes, (batch_size,), device=device)\n",
    "    return z_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68c7c4-0bbb-4820-9f15-b68546731345",
   "metadata": {},
   "source": [
    "# Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e25b10-9dc7-4434-bd0d-5628564bcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(generator, discriminator, dataset_loader, device, latent_dim, n_epochs=100, n_batch=128, n_classes=10):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    # separate adam optimizers for generator and discriminator\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # lists to track loss\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "\n",
    "    # loop through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_g_accum = 0.0\n",
    "        loss_d_accum = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        loop = tqdm(dataset_loader, leave=True)\n",
    "        for imgs, labels in loop:\n",
    "            current_batch_size = imgs.size(0)\n",
    "            if current_batch_size != n_batch:  # Skip incomplete batches\n",
    "                continue\n",
    "\n",
    "            num_batches += 1\n",
    "            # create real and fake labels for discrminator training\n",
    "            real_labels = torch.ones(current_batch_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(current_batch_size, 1, device=device)\n",
    "\n",
    "            # train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            real_imgs = imgs.to(device)\n",
    "            labels_real = labels.to(device).unsqueeze(1)  # Adjust the label dimension if necessary\n",
    "            real_loss = criterion(discriminator(real_imgs, labels_real), real_labels)\n",
    "\n",
    "            z_input, gen_labels = generate_latent_points(latent_dim, current_batch_size, n_classes, device)\n",
    "            fake_imgs = generator(z_input, gen_labels.unsqueeze(1)).detach()\n",
    "            fake_loss = criterion(discriminator(fake_imgs, gen_labels.unsqueeze(1)), fake_labels)\n",
    "\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            loss_d_accum += d_loss.item()\n",
    "\n",
    "            # train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            gen_imgs = generator(z_input, gen_labels.unsqueeze(1))\n",
    "            g_loss = criterion(discriminator(gen_imgs, gen_labels.unsqueeze(1)), real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "            loss_g_accum += g_loss.item()\n",
    "\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "            loop.set_postfix(D_loss=d_loss.item(), G_loss=g_loss.item())\n",
    "\n",
    "        # calculate and store average losses for generator and discriminator for this epoch\n",
    "        avg_loss_d = loss_d_accum / num_batches\n",
    "        avg_loss_g = loss_g_accum / num_batches\n",
    "        losses_d.append(avg_loss_d)\n",
    "        losses_g.append(avg_loss_g)\n",
    "\n",
    "        # visualization at the end of each epoch\n",
    "        with torch.no_grad():\n",
    "            # generate one latent point per class\n",
    "            z = torch.randn(n_classes, latent_dim, device=device)  # n_classes latent points\n",
    "            labels = torch.arange(0, n_classes, device=device)  # One label for each class\n",
    "            \n",
    "            # generate images\n",
    "            generated_images = generator(z, labels.unsqueeze(1))\n",
    "            display_images(generated_images, labels=None, title=\"\", num_samples=10, cols=10)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'conditional_gan_300.pt')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(losses_g, label=\"Generator\")\n",
    "    plt.plot(losses_d, label=\"Discriminator\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341867ca-a35b-4920-a7f1-c818c8fdf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the latent space\n",
    "latent_dim = 100\n",
    "# Number of classes in the dataset (for FashionMNIST, it's 10)\n",
    "n_classes = 10\n",
    "\n",
    "generator = Generator(latent_dim=latent_dim, n_classes=n_classes).to(device) # Create the generator\n",
    "discriminator = Discriminator(n_classes=n_classes).to(device) # Create the discriminator\n",
    "\n",
    "# Define data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3249c9-0882-4c77-9277-0e46fc608b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "n_batch=128\n",
    "train(generator, discriminator, train_loader, device, latent_dim, n_epochs, n_batch, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8eee7f-ac7b-42de-a919-8bb64d66d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n_samples images of target_class\n",
    "def generate_images_for_class(generator, latent_dim, target_class, n_samples, device):\n",
    "    generator.to(device)\n",
    "    # generate points in the latent space\n",
    "    # z = torch.randn(n_samples, latent_dim, device=device)\n",
    "    z, _ = generate_latent_points(latent_dim, n_samples, n_classes=10, device=device)\n",
    "    # create labels for the target class\n",
    "    labels = torch.full((n_samples,), target_class, dtype=torch.long, device=device)\n",
    "    # generate and return images\n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(z, labels)\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15024b-257e-49a9-aa88-bf6d536c179b",
   "metadata": {},
   "source": [
    "# Targeted sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8032d-efb8-4c1b-b322-5fa03e51c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "# Example usage\n",
    "target_class = 1  # For example, class '3'\n",
    "n_samples = 10\n",
    "generated_images = generate_images_for_class(generator, latent_dim, target_class, n_samples, device)\n",
    "\n",
    "display_images(generated_images, labels=None, title=\"\", num_samples=10, cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff737e81-6c65-4ed0-b8e2-ef9811644dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option to load in saved model\n",
    "# latent_dim = 100\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Generator(latent_dim)\n",
    "# model.load_state_dict(torch.load('model path'))\n",
    "# model.eval()  # Set the generator to evaluation mode\n",
    "# model.to(device)\n",
    "\n",
    "all_generated = []\n",
    "\n",
    "for i in range(10):\n",
    "    target_class = i  # For example, class '3'\n",
    "    n_samples = 5\n",
    "    generated_images = generate_images_for_class(generator, latent_dim, target_class, n_samples, device)\n",
    "    all_generated.append(generated_images)\n",
    "\n",
    "all_generated = torch.cat(all_generated, dim=0)\n",
    "display_images(all_generated, labels=None, title=\"\", num_samples=n_samples*10, cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22864a51-47f4-44f2-955f-c457e23371b1",
   "metadata": {},
   "source": [
    "# Quantitative Metrics Calculation (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd333fcb-f87b-4b2c-b391-f8d76be58ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, folder):\n",
    "    \"\"\"\n",
    "    Saves the tensor images to the specified folder after ensuring they are in the correct format for saving.\n",
    "    Adjusted for grayscale images.\n",
    "    \"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, batch in enumerate(images):  # Iterate over batches (each batch corresponds to a class)\n",
    "        for j, img_tensor in enumerate(batch):  # Iterate over images in each batch\n",
    "            # Ensure the image is in CPU memory and squeeze to remove the channel dimension for grayscale\n",
    "            img = img_tensor.cpu().squeeze().numpy()\n",
    "            plt.imsave(os.path.join(folder, f'class_{i}_image_{j}.png'), img, cmap='gray')\n",
    "\n",
    "def compute_fid(real_images_path, fake_images_path):\n",
    "    \"\"\"\n",
    "    Computes the FID score between two sets of images located at the given paths.\n",
    "    \"\"\"\n",
    "    metrics = calculate_metrics(input1=real_images_path, input2=fake_images_path, cuda=True, isc=False, fid=True, kid=False)\n",
    "    return metrics['frechet_inception_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78369c1-11a1-4839-bada-c2060e6dc846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to load in saved model\n",
    "# latent_dim = 100\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Generator(latent_dim)\n",
    "# model.load_state_dict(torch.load('path to .pt file'))\n",
    "# model.eval()  # Set the generator to evaluation mode\n",
    "# model.to(device)\n",
    "\n",
    "all_generated_images = []\n",
    "\n",
    "n_samples = 5000\n",
    "n_classes = 10\n",
    "\n",
    "# generate and collect images\n",
    "with torch.no_grad():\n",
    "    for target_class in range(n_classes):\n",
    "        # generate latent points and corresponding labels for the target class\n",
    "        z, labels = generate_latent_points(latent_dim, n_samples, n_classes, device)\n",
    "        \n",
    "        # since we need images for a specific class, adjust labels to be all 'target_class'\n",
    "        labels.fill_(target_class)\n",
    "        \n",
    "        # generate images using the model,the generated z, and the labels we specified\n",
    "        generated_images = generator(z, labels.unsqueeze(1))\n",
    "        all_generated_images.append(generated_images)\n",
    "\n",
    "# concatenate all images along the first dimension\n",
    "all_generated_images = torch.cat(all_generated_images)\n",
    "\n",
    "# Save the images\n",
    "save_images(all_generated_images, \"generated_images_cdcgan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7397aa3-8727-43d2-9bd0-0d2aef97e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "dataset = datasets.FashionMNIST(root=\"../data\", train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "n_samples = 5000\n",
    "n_classes = 10\n",
    "\n",
    "# initialize a list to hold selected images ensuring n_samples images per class\n",
    "selected_images = []\n",
    "\n",
    "for class_id in range(n_classes):\n",
    "    # Filter indices for the current class\n",
    "    class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_id]\n",
    "    # Randomly select n_samples indices for the current class without replacement\n",
    "    selected_indices = np.random.choice(class_indices, n_samples, replace=False)\n",
    "    \n",
    "    # Append selected images to the list\n",
    "    for idx in selected_indices:\n",
    "        image, _ = dataset[idx]\n",
    "        selected_images.append(image)\n",
    "\n",
    "# Convert list of selected images to a tensor\n",
    "real_images = torch.stack(selected_images)\n",
    "\n",
    "# Save the selected images\n",
    "save_images(real_images, \"real_images_cdcgan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe08dd7-0627-4fd8-9d29-c544b8d5a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FID score\n",
    "fid_score = compute_fid(\"real_images_cdcgan\", \"generated_images_cdcgan\")\n",
    "print(f\"FID Score: {fid_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54957bf8-eb6c-4641-9288-77f925ba785d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
