{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272916a3-3fc1-4ad8-8a26-923acc6ca2a5",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Make sure you have the packages installed. You might need to pip install some of them, the console should tell you if you are missing any packages when you try to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00c218-25c6-471c-9b0f-5b97a114a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch and core machine learning libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Libraries for data loading and pre-processing\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for displaying images\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# libraries used for quantative metrics calculation\n",
    "from torchvision.utils import save_image\n",
    "from torch_fidelity import calculate_metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb000f14-9e2d-4947-9a08-88878d3c598f",
   "metadata": {},
   "source": [
    "# Helper function for displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc67288-3bfe-497b-87c1-88c42e5b8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels=None, title=\"\", num_samples=20, cols=4):\n",
    "    # ensure we don't exceed the number of available images\n",
    "    images = images[:min(num_samples, len(images))]\n",
    "\n",
    "    # create a grid of images\n",
    "    # normalize each image\n",
    "    grid = make_grid(images, nrow=cols, normalize=True, scale_each=True)  # Adjust grid columns\n",
    "    \n",
    "    # convert the grid to a PIL image\n",
    "    grid_img = to_pil_image(grid)\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 12))  # You can adjust the figure size as needed\n",
    "    plt.imshow(grid_img, cmap=\"gray\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # if labels are provided, display them (note: labels arent displayed very well)\n",
    "    if labels is not None:\n",
    "        num_images = len(images)\n",
    "        rows = (num_images + cols - 1) // cols  # Calculate the number of rows in the grid\n",
    "        for i, label in enumerate(labels[:num_images]):\n",
    "            plt.text(\n",
    "                (i % cols) * grid_img.width / cols, \n",
    "                (i // cols + 1) * grid_img.height / rows - 10,  # Adjust text position\n",
    "                label, \n",
    "                horizontalalignment='center',\n",
    "                fontsize=10,\n",
    "                color='white',\n",
    "                weight='bold'\n",
    "            )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a61f4-fb33-4974-b64d-591c7632e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device Type: {device} \" + (f\"| Name: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e12594-684a-41e8-a5fd-4781a8e482fe",
   "metadata": {},
   "source": [
    "# Unconditional Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dd63c-6bc7-471c-bb55-b870675e2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator model to determine if an image is real or fake\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # define the architecture of the discriminator\n",
    "        self.model = nn.Sequential(\n",
    "            # first convolutional downsampling\n",
    "            nn.Conv2d(in_channels, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # second convolutional downsampling\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # flattening the output for the linear\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            # final classifier layer\n",
    "            nn.Linear(128 * 7 * 7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass through the model\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4bed4-3c4c-4069-9ad3-ffda15168860",
   "metadata": {},
   "source": [
    "# Unconditional Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ad351-5fd7-410f-90ec-5bfc455efa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim # the dimensions of the latent noise vector\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # foundation for 7x7 image\n",
    "            nn.Linear(latent_dim, 128 * 7 * 7),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(128 * 7 * 7),\n",
    "            nn.Unflatten(1, (128, 7, 7)),\n",
    "            # first upsampling with fractional strided convolution to 14x14\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # second upsampling with fractional strided convolution to 28x28\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # final convolution to generate the image\n",
    "            nn.Conv2d(128, 1, 7, padding=3, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # forward pass through the model\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b93a2-8151-4af3-a012-99dceb07af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont necessarily need this function but it improves readability\n",
    "# this function takes a value for the latent dimension and the number of samples to generate and creates latent noise vectors\n",
    "# to feed into our GAN for image generation\n",
    "def generate_latent_points(latent_dim, n_samples, device='cpu'):\n",
    "    # generate points in the latent space\n",
    "    x_input = torch.randn(n_samples, latent_dim, device=device)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6db8c-4765-40ed-bfe5-103124db8186",
   "metadata": {},
   "source": [
    "# Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ae232-c6a6-4e98-88f9-6b7c84a68a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(generator, discriminator, dataloader, latent_dim, device=device, n_epochs=100, n_batch=128):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    # initialize separate optimizers for the generator and discriminator networks\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # lists to track loss\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "\n",
    "    # loop through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_g_accum = 0.0\n",
    "        loss_d_accum = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # wrap dataloader with tqdm for a progress bar\n",
    "        loop = tqdm(dataloader, leave=True)\n",
    "        for imgs, _ in loop:\n",
    "            current_batch_size = imgs.size(0)\n",
    "            if current_batch_size != n_batch:  # Skip incomplete batches\n",
    "                continue\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "            real_labels = torch.ones(current_batch_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(current_batch_size, 1, device=device)\n",
    "\n",
    "            # train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            real_imgs = imgs.to(device)\n",
    "            real_loss = criterion(discriminator(real_imgs), real_labels)\n",
    "            noise = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            fake_imgs = generator(noise).detach()\n",
    "            fake_loss = criterion(discriminator(fake_imgs), fake_labels)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            loss_d_accum += d_loss.item()\n",
    "\n",
    "            # train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            gen_imgs = generator(noise)\n",
    "            g_loss = criterion(discriminator(gen_imgs), real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "            loss_g_accum += g_loss.item()\n",
    "\n",
    "            # update the progress bar description\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "            loop.set_postfix(D_loss=d_loss.item(), G_loss=g_loss.item())\n",
    "\n",
    "        # calculate average losses for the current epoch and append to lists\n",
    "        avg_loss_d = loss_d_accum / num_batches\n",
    "        avg_loss_g = loss_g_accum / num_batches\n",
    "        losses_d.append(avg_loss_d)\n",
    "        losses_g.append(avg_loss_g)\n",
    "\n",
    "        # generate and display images at the end of the epoch\n",
    "        with torch.no_grad():\n",
    "            test_noise = torch.randn(10, latent_dim, device=device)\n",
    "            test_imgs = generator(test_noise)\n",
    "            display_images(test_imgs, labels=None, title=\"\", num_samples=10, cols=10)\n",
    "\n",
    "    # save the generator model\n",
    "    torch.save(generator.state_dict(), 'dcgan.pt')\n",
    "\n",
    "    # plot the losses after training\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(losses_g, label=\"Generator\")\n",
    "    plt.plot(losses_d, label=\"Discriminator\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab5a41-8bbe-473d-b176-f61e9a3022fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of the latent space\n",
    "latent_dim = 100\n",
    "\n",
    "# initialize the models\n",
    "discriminator = Discriminator()\n",
    "generator = Generator(latent_dim)\n",
    "\n",
    "# load and pre-process the image data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480244cf-9a5d-4c9a-a7e8-c86051c59e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "n_epochs = 300\n",
    "train(generator, discriminator, train_loader, latent_dim, n_epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814ff73-3244-4c7f-9f1d-01fd18b09aed",
   "metadata": {},
   "source": [
    "# Unconditional sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd54cee-29a8-4d98-83fa-bd0bff7ce113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally load in saved model\n",
    "# latent_dim = 100\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Generator(latent_dim)\n",
    "# model.load_state_dict(torch.load('model path'))\n",
    "# model.eval()  # Set the generator to evaluation mode\n",
    "# model.to(device)\n",
    "\n",
    "# Generate images\n",
    "n_samples = 50 # number of samples to generate\n",
    "#create latent noise vectors\n",
    "latent_points = generate_latent_points(100, n_samples, device=device)\n",
    "with torch.no_grad():\n",
    "    generated_images = generator(latent_points) # generate images\n",
    "\n",
    "# display generated images\n",
    "display_images(generated_images, labels=None, title=\"Unconditional DCGAN Generations\", num_samples=n_samples, cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811debf-4429-40d5-b76e-2e979fb6763e",
   "metadata": {},
   "source": [
    "# Quantative Metrics Calculation (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62603e20-348f-433a-9a34-ec97097ee3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a tensor of images and saves them to a specified folder\n",
    "def save_images(images, folder=\"saved_images\"):\n",
    "    if not isinstance(images, torch.Tensor):\n",
    "        raise ValueError(\"Images should be a PyTorch tensor\")\n",
    "\n",
    "    images = images.detach().cpu()\n",
    "\n",
    "    # normalize and prepare images\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, img_tensor in enumerate(images):\n",
    "        img = img_tensor.squeeze()  # Remove color channels if 1\n",
    "        # Since imshow handles normalization for display, ensure the saved images mimic this display behavior\n",
    "        # here, assuming images are normalized to [0, 1] for grayscale as in show_images\n",
    "        img_np = img.numpy()\n",
    "        plt.imsave(os.path.join(folder, f'image_{i}.png'), img_np, cmap='gray')\n",
    "\n",
    "# computes the FID score between two sets of images at the passed paths\n",
    "def compute_fid(real_images_path, fake_images_path):\n",
    "    metrics = calculate_metrics(input1=real_images_path, input2=fake_images_path, cuda=True, isc=False, fid=True, kid=False, samples_find_deep=True)\n",
    "    return metrics['frechet_inception_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf0cd6-8ef5-4499-819c-21a9300e729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally load in saved model\n",
    "# latent_dim = 100\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Generator(latent_dim)\n",
    "# model.load_state_dict(torch.load('model path'))\n",
    "# model.eval()  # Set the generator to evaluation mode\n",
    "# model.to(device)\n",
    "\n",
    "n_loops = 5\n",
    "n_samples = 10000\n",
    "\n",
    "for loop in range(n_loops):\n",
    "    # generate n_samples images using your model\n",
    "    latent_points = generate_latent_points(100, n_samples, device=device)\n",
    "    # generate images\n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(latent_points)\n",
    "\n",
    "    # folder to save images in a loop-specific subfolder\n",
    "    loop_folder = os.path.join(\"generated_images_dcgan\", f'set_{loop + 1}')\n",
    "    save_images(generated_images, loop_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c61bc-71f1-4a4e-b04a-48de74afd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FashionMNIST images\n",
    "dataset = datasets.FashionMNIST(root=\"../data\", train=True, transform=ToTensor(), download=True)\n",
    "real_images = torch.stack([dataset[i][0] for i in range(n_samples * n_loops)]) # Stack images to create a single tensor\n",
    "\n",
    "# save the first real n_samples * n_loops images\n",
    "save_images(real_images, \"real_images_dcgan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bbab2b-bb64-40a8-ab3f-e418ac963436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print FID score\n",
    "fid_score = compute_fid(\"real_images_dcgan\", \"generated_images_dcgan\")\n",
    "print(f\"FID Score: {fid_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
