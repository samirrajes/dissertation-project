{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5489467e-0501-4e62-a3e1-5aa7d8a48bf9",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Make sure you have the packages installed. You might need to pip install some of them, the console should tell you if you are missing any packages when you try to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e371680-8a11-46d0-9824-53b0fc1a2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# allows flexible tensor manipulation\n",
    "import einops\n",
    "\n",
    "# progress bars during training and sampling\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch for general machine learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# help in displaying images\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# libraries for data and dataset processing\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torchvision.datasets.mnist import FashionMNIST\n",
    "\n",
    "# libraries for quantative metrics calculations\n",
    "from torchvision.utils import save_image\n",
    "from torch_fidelity import calculate_metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef5682-430b-466d-8a77-89fe81fd4554",
   "metadata": {},
   "source": [
    "# Helper function to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d8762-99c0-4c28-9a2d-532650684cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels=None, title=\"\", num_samples=20, cols=4):\n",
    "    # ensure we don't exceed the number of available images\n",
    "    images = images[:min(num_samples, len(images))]\n",
    "\n",
    "    # create a grid of images\n",
    "    # normalize each image\n",
    "    grid = make_grid(images, nrow=cols, normalize=True, scale_each=True)  # Adjust grid columns\n",
    "    \n",
    "    # convert the grid to a PIL image\n",
    "    grid_img = to_pil_image(grid)\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 12))  # You can adjust the figure size as needed\n",
    "    plt.imshow(grid_img, cmap=\"gray\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # if labels are provided, display them (note: labels arent displayed very well)\n",
    "    if labels is not None:\n",
    "        num_images = len(images)\n",
    "        rows = (num_images + cols - 1) // cols  # Calculate the number of rows in the grid\n",
    "        for i, label in enumerate(labels[:num_images]):\n",
    "            plt.text(\n",
    "                (i % cols) * grid_img.width / cols, \n",
    "                (i // cols + 1) * grid_img.height / rows - 10,  # Adjust text position\n",
    "                label, \n",
    "                horizontalalignment='center',\n",
    "                fontsize=10,\n",
    "                color='white',\n",
    "                weight='bold'\n",
    "            )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821e2b6-8a0f-4512-bd97-1bcff24448b6",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516c18b-03a2-49b3-8d66-4bde5ca52134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensor and normalize [-1,1]\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: (x - 0.5) * 2)\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# load FashionMNIST dataset\n",
    "train_dataset = FashionMNIST(\"./data\", download=True, train=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7d1a1-0f98-4242-9a31-3ff4f0b71e72",
   "metadata": {},
   "source": [
    "# Classifier-Free Guidance DDPM Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79a130-3e44-491d-91b8-5ec77dc89ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the device (GPU if available)\n",
    "# this is essential as we want to train on our GPU!\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print the device being used to make sure we have access\n",
    "print(f\"Using device: {device}\\t\" + (f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e7bd7-9792-4be0-b85b-240738facaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance scheduler class to pre-compute noise values\n",
    "class VarianceScheduler:\n",
    "    def __init__(self, beta1, beta2, T, device, schedule_type=\"linear\", s=0.008, beta_max=0.999):\n",
    "        # make sure beta values are in defined bounds\n",
    "        assert 0 < beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "        self.device = device\n",
    "        self.schedule_type = schedule_type\n",
    "        self.beta1 = beta1 # start beta\n",
    "        self.beta2 = beta2 # end beta\n",
    "        self.T = T # total timesteps in diffusion process\n",
    "        self.s = s # smoothing constant for cosine schedule\n",
    "        self.beta_max = beta_max # capped beta value for cosine schedule\n",
    "        self.schedule = self.compute_schedule()\n",
    "\n",
    "    def compute_schedule(self):\n",
    "        timesteps = torch.arange(0, self.T, dtype=torch.float32).to(self.device) # define timesteps 0 to T and move to gpu\n",
    "        if self.schedule_type == 'linear':\n",
    "            #compute linear schedule from Ho et al.'s DDPM paper\n",
    "            betas = torch.linspace(self.beta1, self.beta2, self.T).to(self.device) \n",
    "        elif self.schedule_type == 'cosine':\n",
    "            #compute cosine schedule using equations from Nichol et al.'s Improved DDPM paper\n",
    "            t_scaled = (timesteps / self.T + self.s) / (1 + self.s) * (torch.pi / 2)\n",
    "            f_t = torch.cos(t_scaled).pow(2)\n",
    "            f_0 = torch.cos(torch.tensor(self.s / (1 + self.s) * (torch.pi / 2))).pow(2)\n",
    "            alpha_bars = f_t / f_0\n",
    "            alpha_bars_prev = torch.cat([torch.tensor([1]).to(self.device), alpha_bars[:-1]])\n",
    "            betas = 1 - alpha_bars / alpha_bars_prev\n",
    "            betas = torch.clip(betas, 0, self.beta_max)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown schedule type: {}\".format(self.schedule_type)) # error for unknown schedule type\n",
    "\n",
    "        # calculate alphas and alpha_bars as shown in DDPM paper (reparameterization trick)\n",
    "        alphas = 1 - betas\n",
    "        alpha_bars = torch.tensor([torch.prod(alphas[:i + 1]) for i in range(len(alphas))]).to(self.device)\n",
    "\n",
    "        # return dictionary of betas, alphas, and alpha_bars\n",
    "        schedule = {\n",
    "            \"betas\": betas,\n",
    "            \"alphas\": alphas,\n",
    "            \"alpha_bars\": alpha_bars,\n",
    "        }\n",
    "        return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42702a19-6f99-4d58-9c05-00ebeced73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM class\n",
    "class CFG_DDPM(nn.Module):\n",
    "    def __init__(self, network, T=1000, beta1=10 ** -4, beta2=0.02, schedule_type='linear', device=None):\n",
    "        super(CFG_DDPM, self).__init__()\n",
    "        self.device = device\n",
    "        self.network = network.to(device) # move the unet to the gpu\n",
    "        self.T = T # timesteps T\n",
    "        self.variance_scheduler = VarianceScheduler(beta1, beta2, T, device, schedule_type) # Initialize variance schdeuler\n",
    "\n",
    "        # use register_buffer to store each schedule component\n",
    "        for k, v in self.variance_scheduler.schedule.items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "    # noising function (forward diffusion)\n",
    "    def forward(self, x0, t, noise=None):\n",
    "        # Make input image more noisy (we can directly skip to the desired step)\n",
    "        n, channel, height, width = x0.shape\n",
    "        a_bar = self.alpha_bars[t]\n",
    "\n",
    "        #if no noise is passed then calculate a random noise mask of the same dimensions as the image\n",
    "        if noise is None:\n",
    "            noise = torch.randn(n, channel, height, width).to(self.device) # move to gpu\n",
    "\n",
    "        # noising function\n",
    "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * noise\n",
    "        # return noised image\n",
    "        return noisy\n",
    "\n",
    "    # added label conditioning for CFG\n",
    "    def backward(self, x, t, labels):\n",
    "        return self.network(x, t, labels)  # switch unet to eval mode (stops training behaviors)\n",
    "\n",
    "    # MODIFIED SAMPLING!\n",
    "    # produces n_samples of specified class labels in list labels. Note that the nuimber of provided labels needs to = n_samples\n",
    "    def sample_images(self, labels, n_samples=16, cfg_scale=3.0):\n",
    "        self.network.eval()  # set unet to eval mode\n",
    "\n",
    "        # torch.no_grad() turns off gradient computation (not needed when sampling) for reduced memory usage and faster computations\n",
    "        with torch.no_grad():\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=self.device)\n",
    "            x = torch.randn(n_samples, *(1,28,28)).to(self.device)\n",
    "\n",
    "            # loop backward through entire diffusion process (T-1 -> 0)\n",
    "            for i in tqdm(reversed(range(1, self.T)), position=0):\n",
    "                # create tensor filled with the current timestep, shaped for each sampl\n",
    "                t = (torch.ones(n_samples) * i).long().to(self.device)\n",
    "                \n",
    "                # noise prediction conditioned on class labels\n",
    "                conditional_predicted_noise = self.backward(x, t, labels)\n",
    "                \n",
    "                if cfg_scale > 0:\n",
    "                    # unconditional prediction\n",
    "                    unconditional_predicted_noise = self.backward(x, t, None)\n",
    "                    # linear interpolation between conditional and unconditional noise based on cfg_scale\n",
    "                    predicted_noise = torch.lerp(unconditional_predicted_noise, conditional_predicted_noise, cfg_scale)\n",
    "                \n",
    "                # get alpha, alpha_bar, and beta values for the current timestep from the precomputed schedule\n",
    "                alpha_t = self.alphas[i]\n",
    "                alpha_t_bar = self.alpha_bars[i]\n",
    "                beta_t = self.betas[i]\n",
    "                \n",
    "                # determine noise to be added at this timestep\n",
    "                if i > 1:\n",
    "                    # sample random noise if not at the final timestep\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    # use no noise at the final timestep to avoid adding unnecessary randomness\n",
    "                    noise = torch.zeros_like(x)\n",
    "                \n",
    "                # update image tensor using reverse diffusion formula\n",
    "                x = 1 / torch.sqrt(alpha_t) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_t_bar)) * predicted_noise) + torch.sqrt(beta_t) * noise\n",
    "\n",
    "        # reset unet to training mode\n",
    "        self.network.train()  # Set the model back to train mode\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4ac37-170a-4002-b246-02226d21f877",
   "metadata": {},
   "source": [
    "# Conditional U-Net\n",
    "We now build upon the U-Net that we used in our unconditional DDPM implementation \\\n",
    "We still acknowledge the GitHub implementation which our unconditional model was heavily inspired by, as it was a foundational step in coding the conditional diffusion model. (Cited in dissertation report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba9664-2f86-40d0-9e87-4ab37e16021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net helpers\n",
    "\n",
    "def sinusoidal_embedding(n, dim):\n",
    "    # returns the standard positional embedding\n",
    "    if not isinstance(n, int) or not isinstance(dim, int) or n < 1 or dim < 1:\n",
    "        raise ValueError(\"both 'n' and 'dim' must be positive integers!\")\n",
    "    \n",
    "    # vectorized calculation of the frequency terms\n",
    "    wk = torch.pow(10_000, -torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    "    wk = wk.reshape(1, -1)\n",
    "    # calculate positional information\n",
    "    t = torch.arange(n, dtype=torch.float32).reshape(n, 1)\n",
    "    \n",
    "    # create the embedding matrix\n",
    "    embedding = torch.zeros(n, dim)\n",
    "    embedding[:, 0::2] = torch.sin(t * wk)\n",
    "    embedding[:, 1::2] = torch.cos(t * wk)\n",
    "\n",
    "    return embedding\n",
    " \n",
    "# embedding and label embedding are concatenated before being run through the MLP\n",
    "def make_context_embedding(embedding_dim, out_features):\n",
    "    \"\"\"Utility function to create time embedding layers.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, out_features),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(out_features, out_features)\n",
    "    )\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, shape, in_channels, out_channels, kernel=3, stride=1, padding=1, activation=None, normalize=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(shape)\n",
    "        self.convolution1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding)\n",
    "        self.convolution2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding)\n",
    "        self.activation = nn.SiLU() if activation is None else activation\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layernorm(x) if self.normalize else x\n",
    "        out = self.convolution1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.convolution2(out)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c056d5f-66c5-45aa-822a-aa89a8486c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, T=1000, embedding_dim=100, num_classes=10):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # initialize our sinusoidal time embedding\n",
    "        self.time_embed = nn.Embedding(T, embedding_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(T, embedding_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "\n",
    "        # make context embeddings for every layer\n",
    "        self.context_embedding1 = make_context_embedding(embedding_dim, 1)\n",
    "        self.context_embedding2 = make_context_embedding(embedding_dim, 10)\n",
    "        self.context_embedding3 = make_context_embedding(embedding_dim, 20)\n",
    "        self.context_embedding_mid = make_context_embedding(embedding_dim, 40)\n",
    "        self.context_embedding4 = make_context_embedding(embedding_dim, 80)\n",
    "        self.context_embedding5 = make_context_embedding(embedding_dim, 40)\n",
    "        self.context_embedding_out = make_context_embedding(embedding_dim, 20)\n",
    "\n",
    "        # create a class label embedding\n",
    "        if num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "        # first residual convolutional block, made up of 3 convolutional blocks\n",
    "        # increases our feature depth from 1 to 10\n",
    "        self.residual1 = nn.Sequential(\n",
    "            ConvBlock((1, 28, 28), 1, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10)\n",
    "        )\n",
    "        \n",
    "        # first downsampling, decreases image dimensions to (14x14)\n",
    "        self.downsample1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
    "\n",
    "        # second residual convolutional block, increases feature depth to 20\n",
    "        self.residual2 = nn.Sequential(\n",
    "            ConvBlock((10, 14, 14), 10, 20),\n",
    "            ConvBlock((20, 14, 14), 20, 20),\n",
    "            ConvBlock((20, 14, 14), 20, 20)\n",
    "        )\n",
    "\n",
    "        # second downsampling, decreases image dimensions to 7x7\n",
    "        self.downsample2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "\n",
    "        # third residual convolutional block, increases feature depth to 40\n",
    "        self.residual3 = nn.Sequential(\n",
    "            ConvBlock((20, 7, 7), 20, 40),\n",
    "            ConvBlock((40, 7, 7), 40, 40),\n",
    "            ConvBlock((40, 7, 7), 40, 40)\n",
    "        )\n",
    "\n",
    "        # third downsampling decreases spatial dimensions to 3x3\n",
    "        self.downsample3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(40, 40, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "        # bottleneck, decreases feature depth to 20 and then increases back to 40\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBlock((40, 3, 3), 40, 20),\n",
    "            ConvBlock((20, 3, 3), 20, 20),\n",
    "            ConvBlock((20, 3, 3), 20, 40)\n",
    "        )\n",
    "\n",
    "        # first upsampling, increase the spatial dimensions to 7x7\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 40, 4, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
    "        )\n",
    "\n",
    "        # fourth residual convolutional block, decreases feature depth from 80 to 20\n",
    "        # the feature depth is 80 as we concatenate the correponding downsampling layers data to this layer\n",
    "        # via skip connection\n",
    "        self.residual4 = nn.Sequential(\n",
    "            ConvBlock((80, 7, 7), 80, 40),\n",
    "            ConvBlock((40, 7, 7), 40, 20),\n",
    "            ConvBlock((20, 7, 7), 20, 20)\n",
    "        )\n",
    "\n",
    "        # second upsampling, increase the spatial dimensions to 14x14\n",
    "        self.upsample2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "\n",
    "        # fifth residual convolutional block, decreases feature depth from 40 to 20\n",
    "        # the feature depth 40 due to concatenation from downsampling stage\n",
    "        self.residual5 = nn.Sequential(\n",
    "            ConvBlock((40, 14, 14), 40, 20),\n",
    "            ConvBlock((20, 14, 14), 20, 10),\n",
    "            ConvBlock((10, 14, 14), 10, 10)\n",
    "        )\n",
    "\n",
    "        # third upsampling, increase spatial dimensions to 29x29\n",
    "        self.upsample3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
    "        self.residual_out = nn.Sequential(\n",
    "            ConvBlock((20, 28, 28), 20, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10, normalize=False)\n",
    "        )\n",
    "\n",
    "        # final convolution for (10,28,28) -> (1,28,28)\n",
    "        self.final_convolution = nn.Conv2d(10, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t, labels):\n",
    "        time_embedding = self.time_embed(t).squeeze(1)\n",
    "        # print(f\"Adjusted Time embedding shape: {time_embedding.shape}\")\n",
    "\n",
    "        # if a label is passed create a context embedding of both time and label\n",
    "        if labels is not None:\n",
    "            label_embedding = self.label_emb(labels)\n",
    "            # print(f\"Time embedding shape: {time_embedding.shape}\")\n",
    "            # print(f\"Label embedding shape: {label_embedding.shape}\")\n",
    "            context_embedding = time_embedding + label_embedding\n",
    "        else:\n",
    "            # if there is no label then we just condition on temporal information\n",
    "            context_embedding = time_embedding\n",
    "\n",
    "        n = len(x)\n",
    "        out1 = self.residual1(x + self.context_embedding1(context_embedding).reshape(n, -1, 1, 1))\n",
    "        out2 = self.residual2(self.downsample1(out1) + self.context_embedding2(context_embedding).reshape(n, -1, 1, 1)) \n",
    "        out3 = self.residual3(self.downsample2(out2) + self.context_embedding3(context_embedding).reshape(n, -1, 1, 1))\n",
    "\n",
    "        out_mid = self.bottleneck(self.downsample3(out3) + self.context_embedding_mid(context_embedding).reshape(n, -1, 1, 1))\n",
    "\n",
    "        # skip connection\n",
    "        out4 = torch.cat((out3, self.upsample1(out_mid)), dim=1)  # (N, 80, 7, 7) \n",
    "        out4 = self.residual4(out4 + self.context_embedding4(context_embedding).reshape(n, -1, 1, 1))\n",
    "\n",
    "        # skip connection\n",
    "        out5 = torch.cat((out2, self.upsample2(out4)), dim=1)  # (N, 40, 14, 14)\n",
    "        out5 = self.residual5(out5 + self.context_embedding5(context_embedding).reshape(n, -1, 1, 1))\n",
    "\n",
    "        # skip connection\n",
    "        out = torch.cat((out1, self.upsample3(out5)), dim=1)  # (N, 20, 28, 28)\n",
    "        out = self.residual_out(out + self.context_embedding_out(context_embedding).reshape(n, -1, 1, 1))\n",
    "\n",
    "        out = self.final_convolution(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6504e-6642-4960-abc0-78b5f71cc348",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abacdf-7781-4b91-8e18-7071a6ba0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(ddpm, dataloader, n_epochs, optim, device, sample_images=True, store=\"ddpm.pt\"):\n",
    "    T = ddpm.T\n",
    "    mse = nn.MSELoss() #loss\n",
    "    best_loss = float(\"inf\") # we initialize a best loss tracker\n",
    "    losses = [] # a store for losses to plot\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Training progress\"):\n",
    "        epoch_loss = 0.0\n",
    "        for step, (images, labels) in enumerate(tqdm(dataloader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\")):\n",
    "            # load data and move to device\n",
    "            x0 = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            n = len(x0)\n",
    "\n",
    "            # pick some noise for each of the images in the batch, a timestep\n",
    "            eta = torch.randn_like(x0).to(device)\n",
    "            t = torch.randint(0, T, (n,)).to(device)\n",
    "\n",
    "            # computing the noisy image based on x0 and the time-step (forward process)\n",
    "            noisy_imgs = ddpm(x0, t, eta)\n",
    "\n",
    "            # 10% of the time, we should drop conditioning on labels\n",
    "            if np.random.random() < 0.1:\n",
    "                labels = None\n",
    "                \n",
    "            # getting model estimation of noise based on the images, timesteps, and labels\n",
    "            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1), labels)\n",
    "\n",
    "            # optimize the MSE between the actual noise and the predicted noise\n",
    "            loss = mse(eta_theta, eta)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # calculates average loss over epoch\n",
    "            epoch_loss += loss.item() * len(x0) / len(dataloader.dataset)\n",
    "        # store epoch loss for plotting\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        # if sample_images then, display images generated at this epoch\n",
    "        if display:\n",
    "            # create a list of labels from 0 to 9 for targeted sampling\n",
    "            desired_labels = list(range(10))\n",
    "            # use the sample_images method from the DDPM class\n",
    "            generated_images = ddpm.sample_images(labels=desired_labels, n_samples=10)\n",
    "            # display the generated images\n",
    "            display_images(generated_images, labels=None, title=f\"Images generated at epoch {epoch + 1}\", num_samples=10, cols=10)\n",
    "        \n",
    "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
    "\n",
    "        # store the model if we attain  a new low loss\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(ddpm.state_dict(), store)\n",
    "            log_string += \" model stored\"\n",
    "\n",
    "        print(log_string)  # print epoch loss and whether model was stored\n",
    "        \n",
    "    # Plotting the loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, n_epochs + 1), losses, marker='o', linestyle='-')\n",
    "    plt.title(\"Training Loss Per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e749277-b32b-42e4-947f-6b909bac38fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model\n",
    "# values defined as originally suggested by the ddpm paper authors\n",
    "T = 1000\n",
    "beta1 = 10 ** -4\n",
    "beta2 = 0.02\n",
    "schedule_type = \"linear\"\n",
    "num_classes = 10 # 10 classes in FashionMNIST\n",
    "\n",
    "# initialize ddpm for training\n",
    "ddpm = CFG_DDPM(UNet(T, num_classes=num_classes), T=T, beta1=beta1, beta2=beta2, schedule_type=schedule_type, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5ca0-8cc8-4513-aef7-5251eb4713dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# we save the best loss model and the final 300 epoch trained model\n",
    "store = \"best_loss_ddpm_cfg.pt\"\n",
    "epochs = 300\n",
    "lr = 0.001\n",
    "\n",
    "training_loop(ddpm, train_loader, epochs, optim=Adam(ddpm.parameters(), lr), device=device, sample_images=True, store=store)\n",
    "torch.save(ddpm.state_dict(), 'ddpm_cfg.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5313889-dfe7-4363-b163-1f4d37cd8709",
   "metadata": {},
   "source": [
    "# Targeted sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afb229-2ca2-4b59-8c71-7c0904b9a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load a pre-trained model here.\n",
    "# T = 1000\n",
    "# beta1 = 10 ** -4\n",
    "# beta2 = 0.02\n",
    "# schedule_type = \"linear\"\n",
    "# num_classes = 10\n",
    "# model = CFG_DDPM(UNet(T, num_classes=num_classes), T=T, beta1=beta1, beta2=beta2, device=device, schedule_type=schedule_type)\n",
    "# model.load_state_dict(torch.load(model path))\n",
    "\n",
    "desired_labels = []\n",
    "n_samples = 10\n",
    "for i in range (10):\n",
    "    desired_labels += ([i]*10)\n",
    "\n",
    "generated_images = ddpm.sample_images(labels=desired_labels, n_samples=100)\n",
    "display_images(generated_images, labels=None, title=\"Unconditional Diffusion Generations\", num_samples=100, cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704cf16-5da6-432e-a802-bdca2554c63a",
   "metadata": {},
   "source": [
    "# Quantitative Metrics Calculation (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0588bb-6df3-4da4-9253-e88ce8946370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, folder=\"saved_images\"):\n",
    "    # takes a tensor of images and saves them in a specified folder\n",
    "    if not isinstance(images, torch.Tensor):\n",
    "        raise ValueError(\"Images should be a PyTorch tensor\")\n",
    "\n",
    "    images = images.detach().cpu()\n",
    "\n",
    "    # normalize and prepare images\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, img_tensor in enumerate(images):\n",
    "        img = img_tensor.squeeze()  # remove color channels\n",
    "        img_np = img.numpy()\n",
    "        plt.imsave(os.path.join(folder, f'image_{i}.png'), img_np, cmap='gray')\n",
    "\n",
    "def compute_fid(real_images_path, fake_images_path):\n",
    "    # computes the FID score between two sets of images located at the given folder paths.\n",
    "    # we set isc and kid to false as we only want to calculate FID scores\n",
    "    # we set samples_find_deep=True as we need to find images recursively within our folders\n",
    "    metrics = calculate_metrics(input1=real_images_path, input2=fake_images_path, cuda=True, isc=False, fid=True, kid=False, samples_find_deep=True)\n",
    "    return metrics['frechet_inception_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7f5c4-35aa-45b1-be75-ff13a8b9d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load a pre-trained model here.\n",
    "# T = 1000\n",
    "# beta1 = 10 ** -4\n",
    "# beta2 = 0.02\n",
    "# schedule_type = \"linear\"\n",
    "# num_classes = 10\n",
    "# model = CFG_DDPM(UNet(T, num_classes=num_classes), T=T, beta1=beta1, beta2=beta2, device=device, schedule_type=schedule_type)\n",
    "# model.load_state_dict(torch.load(model path))\n",
    "\n",
    "# number of loops and samples\n",
    "n_loops = 5\n",
    "n_samples_per_class = 1000\n",
    "n_classes = 10\n",
    "total_samples = n_samples_per_class * n_classes\n",
    "\n",
    "# base folder to store generated images\n",
    "base_folder_name = \"generated_images_conditional\"\n",
    "\n",
    "for loop in range(n_loops):\n",
    "    # generate labels for each class\n",
    "    labels = []\n",
    "    for class_label in range(n_classes):\n",
    "        labels += [class_label] * n_samples_per_class\n",
    "\n",
    "    # sample images using the generated labels\n",
    "    generated_images = ddpm.sample_images(labels=labels, n_samples=total_samples)\n",
    "\n",
    "    # folder to save images in a loop-specific subfolder\n",
    "    loop_folder = os.path.join(base_folder_name, f'set_{loop + 1}')\n",
    "    if not os.path.exists(loop_folder):\n",
    "        os.makedirs(loop_folder)\n",
    "    \n",
    "    # save the images in the specified loop folder\n",
    "    save_images(generated_images, loop_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779fb046-012b-4983-8349-5f26cca40895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FashionMNIST dataset\n",
    "dataset = FashionMNIST(root=\"../data\", train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# initialize a list to hold selected images\n",
    "n_images_per_class = 10000\n",
    "selected_images = []\n",
    "\n",
    "for class_id in range(10):\n",
    "    # filter indices for the current class\n",
    "    class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_id]\n",
    "    # Randomly select 5000 indices for the current class without replacement\n",
    "    selected_indices = np.random.choice(class_indices, n_images_per_class, replace=False)\n",
    "    \n",
    "    # append selected images to the list\n",
    "    for idx in selected_indices:\n",
    "        image, _ = dataset[idx]\n",
    "        selected_images.append(image)\n",
    "\n",
    "# convert list of selected images to a tensor\n",
    "real_images = torch.stack(selected_images)\n",
    "\n",
    "# save the selected images\n",
    "save_images(real_images, \"real_images_conditional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204f234-de2e-4a6d-8bc1-a568f0ba49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print FID score of synthetic vs authentic images\n",
    "fid_score = compute_fid(\"real_images_conditional\", \"generated_images_conditional\")\n",
    "print(f\"FID Score: {fid_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
