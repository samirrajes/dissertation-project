{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4968955-f097-4695-900f-85ac945fafa4",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Make sure you have the packages installed. You might need to pip install some of them, the console should tell you if you are missing any packages when you try to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e371680-8a11-46d0-9824-53b0fc1a2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# allows flexible tensor manipulation\n",
    "import einops\n",
    "\n",
    "# progress bars during training and sampling\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch for general machine learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# help in displaying images\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# libraries for data and dataset processing\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torchvision.datasets.mnist import FashionMNIST\n",
    "\n",
    "# libraries for quantative metrics calculations\n",
    "from torchvision.utils import save_image\n",
    "from torch_fidelity import calculate_metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5bee78-32ac-4873-8d3e-42663d55f7d0",
   "metadata": {},
   "source": [
    "# Helper function to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d8762-99c0-4c28-9a2d-532650684cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels=None, title=\"\", num_samples=20, cols=4):\n",
    "    # ensure we don't exceed the number of available images\n",
    "    images = images[:min(num_samples, len(images))]\n",
    "\n",
    "    # create a grid of images\n",
    "    # normalize each image\n",
    "    grid = make_grid(images, nrow=cols, normalize=True, scale_each=True)  # Adjust grid columns\n",
    "    \n",
    "    # convert the grid to a PIL image\n",
    "    grid_img = to_pil_image(grid)\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 12))  # You can adjust the figure size as needed\n",
    "    plt.imshow(grid_img, cmap=\"gray\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # if labels are provided, display them (note: labels arent displayed very well)\n",
    "    if labels is not None:\n",
    "        num_images = len(images)\n",
    "        rows = (num_images + cols - 1) // cols  # Calculate the number of rows in the grid\n",
    "        for i, label in enumerate(labels[:num_images]):\n",
    "            plt.text(\n",
    "                (i % cols) * grid_img.width / cols, \n",
    "                (i // cols + 1) * grid_img.height / rows - 10,  # Adjust text position\n",
    "                label, \n",
    "                horizontalalignment='center',\n",
    "                fontsize=10,\n",
    "                color='white',\n",
    "                weight='bold'\n",
    "            )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4b2c3-461f-48c1-9b94-ab5f26c8e29b",
   "metadata": {},
   "source": [
    "# Pre-process data (and display FashionMNIST images)\n",
    "\n",
    "Here we can have a look at what some of the real FashionMNIST images look like, allowing us to get a sense of the types of images we will be generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516c18b-03a2-49b3-8d66-4bde5ca52134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensor and normalize [-1,1]\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: (x - 0.5) * 2)\n",
    "])\n",
    "\n",
    "# we utilize a batch_size of 128\n",
    "batch_size = 128\n",
    "\n",
    "# load FashionMNIST dataset from torchvision and normalize it [-1,1]\n",
    "train_dataset = FashionMNIST(\"../data\", download=True, train=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialize label names for display in relation to their class label\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#get the next batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# convert labels to their corresponding names for display\n",
    "label_texts = [label_names[label] for label in labels]\n",
    "\n",
    "# display images with labels\n",
    "display_images(images, labels=None, title=None, num_samples=50, cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734a5ea-a52a-42eb-8448-70ab7c4b0400",
   "metadata": {},
   "source": [
    "# Unconditional DDPM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79a130-3e44-491d-91b8-5ec77dc89ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the device (GPU if available)\n",
    "# this is essential as we want to train on our GPU!\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device Type: {device} \" + (f\"| Name: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d9084-f306-4c11-93db-874447860cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance scheduler class to pre-compute noise values\n",
    "class VarianceScheduler:\n",
    "    def __init__(self, beta1, beta2, T, device, schedule_type=\"linear\", s=0.008, beta_max=0.999):\n",
    "        # make sure beta values are in defined bounds\n",
    "        assert 0 < beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "        self.device = device\n",
    "        self.schedule_type = schedule_type\n",
    "        self.beta1 = beta1 # start beta\n",
    "        self.beta2 = beta2 # end beta\n",
    "        self.T = T # total timesteps in diffusion process\n",
    "        self.s = s # smoothing constant for cosine schedule\n",
    "        self.beta_max = beta_max # capped beta value for cosine schedule\n",
    "        self.schedule = self.compute_schedule()\n",
    "\n",
    "    def compute_schedule(self):\n",
    "        timesteps = torch.arange(0, self.T, dtype=torch.float32).to(self.device) # define timesteps 0 to T and move to gpu\n",
    "        if self.schedule_type == 'linear':\n",
    "            # compute linear schedule from Ho et al.'s DDPM paper\n",
    "            betas = torch.linspace(self.beta1, self.beta2, self.T).to(self.device) \n",
    "        elif self.schedule_type == 'cosine':\n",
    "            # compute cosine schedule using equations from Nichol et al.'s Improved DDPM paper\n",
    "            t_scaled = (timesteps / self.T + self.s) / (1 + self.s) * (torch.pi / 2)\n",
    "            f_t = torch.cos(t_scaled).pow(2)\n",
    "            f_0 = torch.cos(torch.tensor(self.s / (1 + self.s) * (torch.pi / 2))).pow(2)\n",
    "            alpha_bars = f_t / f_0\n",
    "            alpha_bars_prev = torch.cat([torch.tensor([1]).to(self.device), alpha_bars[:-1]])\n",
    "            betas = 1 - alpha_bars / alpha_bars_prev\n",
    "            betas = torch.clip(betas, 0, self.beta_max)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown schedule type: {}\".format(self.schedule_type)) # error for unknown schedule type\n",
    "\n",
    "        # calculate alphas and alpha_bars as shown in DDPM paper (reparameterization trick)\n",
    "        alphas = 1 - betas\n",
    "        alpha_bars = torch.tensor([torch.prod(alphas[:i + 1]) for i in range(len(alphas))]).to(self.device)\n",
    "\n",
    "        # return dictionary of betas, alphas, and alpha_bars\n",
    "        schedule = {\n",
    "            \"betas\": betas,\n",
    "            \"alphas\": alphas,\n",
    "            \"alpha_bars\": alpha_bars,\n",
    "        }\n",
    "        return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac662a-cfb3-415d-855e-a5696a11785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the alpha bars of the cosine and linear schedulers to check if they work correctly\n",
    "def plot_alpha_bars(schedulers):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for scheduler in schedulers:\n",
    "        alpha_bars = scheduler.schedule['alpha_bars'].cpu().numpy()\n",
    "        timesteps = torch.arange(0, scheduler.T, dtype=torch.float32).cpu().numpy()\n",
    "        plt.plot(timesteps, alpha_bars, label=f'{scheduler.schedule_type.capitalize()} Scheduler')\n",
    "\n",
    "    plt.title('Comparison of $\\\\alpha_{\\\\bar{t}}$ over Time for Different Schedulers')\n",
    "    plt.xlabel('Diffusion Timestep')\n",
    "    plt.ylabel('$\\\\alpha_{\\\\bar{t}}$')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# initialize schedulers with values from relevant literature\n",
    "scheduler_linear = VarianceScheduler(0.0001, 0.02, 1000, device, 'linear')\n",
    "scheduler_cosine = VarianceScheduler(0.0001, 0.02, 1000, device, 'cosine')\n",
    "\n",
    "plot_alpha_bars([scheduler_linear, scheduler_cosine]) # plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42702a19-6f99-4d58-9c05-00ebeced73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM class (contains our noising, and sampling functions)\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, network, T=1000, beta1=10 ** -4, beta2=0.02, schedule_type='linear', device=None):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.device = device\n",
    "        self.T = T # timesteps T\n",
    "        self.variance_scheduler = VarianceScheduler(beta1, beta2, T, device, schedule_type) # initialize variance schdeuler\n",
    "        self.network = network.to(device) # move the unet to the gpu\n",
    "        \n",
    "        # use register_buffer to store each schedule component\n",
    "        for k, v in self.variance_scheduler.schedule.items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "    # noising function (forward diffusion)\n",
    "    def forward(self, x0, t, noise=None):\n",
    "        # Make input image more noisy (we can directly skip to the desired step)\n",
    "        n, channel, height, width = x0.shape\n",
    "        a_bar = self.alpha_bars[t]\n",
    "\n",
    "        #if no noise is passed then calculate a random noise mask of the same dimensions as the image\n",
    "        if noise is None:\n",
    "            noise = torch.randn(n, channel, height, width).to(self.device) # move to gpu\n",
    "\n",
    "        # noising function\n",
    "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * noise\n",
    "        # return noised image\n",
    "        return noisy\n",
    "\n",
    "    # run each image through the unet for each timestep t in vector t.\n",
    "    def backward(self, x, t):\n",
    "        return self.network(x, t)# the unet returns its prediction of noise added at timestep t\n",
    "\n",
    "    # sampling function that returns n_samples images\n",
    "    def sample_images(self, n_samples=16):\n",
    "        self.network.eval()  # switch unet to eval mode (stops training behaviors)\n",
    "\n",
    "        # torch.no_grad() turns off gradient computation (not needed when sampling) for reduced memory usage and faster computations\n",
    "        with torch.no_grad():\n",
    "            # init image tensor with random noise. shape = (N,1,28,28)\n",
    "            x = torch.randn(n_samples, *(1,28,28)).to(self.device) # move to gpu\n",
    "            \n",
    "            # loop backward through entire diffusion process (T-1 -> 0)\n",
    "            for i in tqdm(reversed(range(1, self.T)), position=0):\n",
    "                # create tensor filled with the current timestep, shaped for each sample\n",
    "                t = (torch.ones(n_samples) * i).long().to(self.device)\n",
    "                \n",
    "                # predict noise added\n",
    "                predicted_noise = self.backward(x, t)\n",
    "                \n",
    "                # get alpha, alpha_bar, and beta values for the current timestep from the precomputed schedule\n",
    "                alpha_t = self.alphas[i]\n",
    "                alpha_t_bar = self.alpha_bars[i]\n",
    "                beta_t = self.betas[i]\n",
    "                \n",
    "                # determine noise to be added at this timestep\n",
    "                if i > 1:\n",
    "                    # sample random noise if not at the final timestep\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    # use no noise at the final timestep to avoid adding unnecessary randomness\n",
    "                    noise = torch.zeros_like(x)\n",
    "                \n",
    "                # update image tensor using reverse diffusion formula\n",
    "                x = 1 / torch.sqrt(alpha_t) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_t_bar)) * predicted_noise) + torch.sqrt(beta_t) * noise\n",
    "            \n",
    "        # reset unet to training mode\n",
    "        self.network.train()\n",
    "        \n",
    "        # return tensor containing image samples\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747e430-3f27-4545-9b55-c1982ea214ae",
   "metadata": {},
   "source": [
    "# U-Net\n",
    "The code for our unconditional U-Net heavily inspired by existing GitHub Implementation (Cited in dissertation report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133c502-d153-4a15-b2a7-cc7df816dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net helpers\n",
    "\n",
    "def sinusoidal_embedding(n, dim):\n",
    "    # returns the standard positional embedding\n",
    "    if not isinstance(n, int) or not isinstance(dim, int) or n < 1 or dim < 1:\n",
    "        raise ValueError(\"both 'n' and 'dim' must be positive integers!\")\n",
    "    \n",
    "    # vectorized calculation of the frequency terms\n",
    "    wk = torch.pow(10_000, -torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    "    wk = wk.reshape(1, -1)\n",
    "    # calculate positional information\n",
    "    t = torch.arange(n, dtype=torch.float32).reshape(n, 1)\n",
    "    \n",
    "    # create the embedding matrix\n",
    "    embedding = torch.zeros(n, dim)\n",
    "    embedding[:, 0::2] = torch.sin(t * wk)\n",
    "    embedding[:, 1::2] = torch.cos(t * wk)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# runs the sinusoidal time embedding through a MLP altering its dimensions to fit the respective unet layer\n",
    "def make_time_embedding(time_emb_dim, out_features):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(time_emb_dim, out_features),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(out_features, out_features)\n",
    "    )\n",
    "\n",
    "# a small convolutional block that applies 2 convolutional operations with options for normalization and specifying activation\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, shape, in_channels, out_channels, kernel=3, stride=1, padding=1, activation=None, normalize=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(shape)\n",
    "        self.convolution1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding)\n",
    "        self.convolution2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding)\n",
    "        self.activation = nn.SiLU() if activation is None else activation\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layernorm(x) if self.normalize else x\n",
    "        out = self.convolution1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.convolution2(out)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94c739-1e63-49d8-8122-16059a482f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, T=1000, time_emb_dim=100):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # initialize sinusoidal time embedding\n",
    "        self.time_embed = nn.Embedding(T, time_emb_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(T, time_emb_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "\n",
    "        # make time embeddings for every layer\n",
    "        self.time_embedding1 = make_time_embedding(time_emb_dim, 1)\n",
    "        self.time_embedding2 = make_time_embedding(time_emb_dim, 10)\n",
    "        self.time_embedding3 = make_time_embedding(time_emb_dim, 20)\n",
    "        self.time_embedding_mid = make_time_embedding(time_emb_dim, 40)\n",
    "        self.time_embedding4 = make_time_embedding(time_emb_dim, 80)\n",
    "        self.time_embedding5 = make_time_embedding(time_emb_dim, 40)\n",
    "        self.time_embedding_out = make_time_embedding(time_emb_dim, 20)\n",
    "\n",
    "        # first residual convolutional block, made up of 3 convolutional blocks\n",
    "        # increases our feature depth from 1 to 10\n",
    "        self.residual1 = nn.Sequential(\n",
    "            ConvBlock((1, 28, 28), 1, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10)\n",
    "        )\n",
    "        \n",
    "        # first downsampling, decreases image dimensions to (14x14)\n",
    "        self.downsample1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
    "\n",
    "        # second residual convolutional block, increases feature depth to 20\n",
    "        self.residual2 = nn.Sequential(\n",
    "            ConvBlock((10, 14, 14), 10, 20),\n",
    "            ConvBlock((20, 14, 14), 20, 20),\n",
    "            ConvBlock((20, 14, 14), 20, 20)\n",
    "        )\n",
    "\n",
    "        # second downsampling, decreases image dimensions to 7x7\n",
    "        self.downsample2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "\n",
    "        # third residual convolutional block, increases feature depth to 40\n",
    "        self.residual3 = nn.Sequential(\n",
    "            ConvBlock((20, 7, 7), 20, 40),\n",
    "            ConvBlock((40, 7, 7), 40, 40),\n",
    "            ConvBlock((40, 7, 7), 40, 40)\n",
    "        )\n",
    "\n",
    "        # third downsampling decreases spatial dimensions to 3x3\n",
    "        self.downsample3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(40, 40, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "        # bottleneck, decreases feature depth to 20 and then increases back to 40\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBlock((40, 3, 3), 40, 20),\n",
    "            ConvBlock((20, 3, 3), 20, 20),\n",
    "            ConvBlock((20, 3, 3), 20, 40)\n",
    "        )\n",
    "\n",
    "        # first upsampling, increase the spatial dimensions to 7x7\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 40, 4, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
    "        )\n",
    "\n",
    "        # fourth residual convolutional block, decreases feature depth from 80 to 20\n",
    "        # the feature depth is 80 as we concatenate the correponding downsampling layers data to this layer\n",
    "        # via skip connection\n",
    "        self.residual4 = nn.Sequential(\n",
    "            ConvBlock((80, 7, 7), 80, 40),\n",
    "            ConvBlock((40, 7, 7), 40, 20),\n",
    "            ConvBlock((20, 7, 7), 20, 20)\n",
    "        )\n",
    "\n",
    "        # second upsampling, increase the spatial dimensions to 14x14\n",
    "        self.upsample2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "\n",
    "        # fifth residual convolutional block, decreases feature depth from 40 to 20\n",
    "        # the feature depth 40 due to concatenation from downsampling stage\n",
    "        self.residual5 = nn.Sequential(\n",
    "            ConvBlock((40, 14, 14), 40, 20),\n",
    "            ConvBlock((20, 14, 14), 20, 10),\n",
    "            ConvBlock((10, 14, 14), 10, 10)\n",
    "        )\n",
    "\n",
    "        # third upsampling, increase spatial dimensions to 29x29\n",
    "        self.upsample3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
    "        self.residual_out = nn.Sequential(\n",
    "            ConvBlock((20, 28, 28), 20, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10),\n",
    "            ConvBlock((10, 28, 28), 10, 10, normalize=False)\n",
    "        )\n",
    "\n",
    "        # final convolution for (10,28,28) -> (1,28,28)\n",
    "        self.final_convolution = nn.Conv2d(10, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        time_embedding = self.time_embed(t) #time embedding\n",
    "        n = len(x) # number of images\n",
    "        # note how we integrate the time embedding into the flow\n",
    "        out1 = self.residual1(x + self.time_embedding1(time_embedding).reshape(n, -1, 1, 1))  # (10, 28, 28)\n",
    "        out2 = self.residual2(self.downsample1(out1) + self.time_embedding2(time_embedding).reshape(n, -1, 1, 1))  # (20, 14, 14)\n",
    "        out3 = self.residual3(self.downsample2(out2) + self.time_embedding3(time_embedding).reshape(n, -1, 1, 1))  # (40, 7, 7)\n",
    "\n",
    "        out_mid = self.bottleneck(self.downsample3(out3) + self.time_embedding_mid(time_embedding).reshape(n, -1, 1, 1))  # (40, 3, 3)\n",
    "\n",
    "        # skip connection\n",
    "        out4 = torch.cat((out3, self.upsample1(out_mid)), dim=1)  # (80, 7, 7)\n",
    "        out4 = self.residual4(out4 + self.time_embedding4(time_embedding).reshape(n, -1, 1, 1))  # (20, 7, 7)\n",
    "\n",
    "        # skip connection\n",
    "        out5 = torch.cat((out2, self.upsample2(out4)), dim=1)  # (40, 14, 14)\n",
    "        out5 = self.residual5(out5 + self.time_embedding5(time_embedding).reshape(n, -1, 1, 1))  # (10, 14, 14)\n",
    "\n",
    "        # skip connection\n",
    "        out = torch.cat((out1, self.upsample3(out5)), dim=1)  # (20, 28, 28)\n",
    "        out = self.residual_out(out + self.time_embedding_out(time_embedding).reshape(n, -1, 1, 1))  # (1, 28, 28)\n",
    "\n",
    "        out = self.final_convolution(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c97bd-2d92-4029-ac29-cf66b1722fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing that the unet outputs accurate information\n",
    "test_unet = UNet()\n",
    "sample_input = torch.randn(1, 1, 28, 28)  # example input tensor\n",
    "time_step = torch.randint(0, 1000, (1,))  # random time step\n",
    "output = test_unet(sample_input, time_step)\n",
    "print(\"Output shape:\", output.shape)  # check output shape\n",
    "# display_images(output, labels=None, title=\"\", num_samples=1, cols=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31590614-c081-45c7-83b2-d6194f9bc2a9",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abacdf-7781-4b91-8e18-7071a6ba0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(ddpm, dataloader, n_epochs, optim, device, sample_images=True, store=\"ddpm.pt\"):\n",
    "    T = ddpm.T\n",
    "    mse = nn.MSELoss() #loss\n",
    "    best_loss = float(\"inf\") # we initialize a best loss tracker\n",
    "    losses = [] # a store for losses to plot\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Training progress\"):\n",
    "        epoch_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(dataloader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\")):\n",
    "            # load data\n",
    "            x0 = batch[0].to(device) \n",
    "            n = len(x0)\n",
    "\n",
    "            # pick some noise for each of the images in the batch, a timestep\n",
    "            eta = torch.randn_like(x0).to(device)\n",
    "            t = torch.randint(0, T, (n,)).to(device)\n",
    "\n",
    "            # compute the noisy image based on the time-step (forward process)\n",
    "            noisy_imgs = ddpm(x0, t, eta)\n",
    "\n",
    "            # getting model estimation of noise based on the images and the time-step\n",
    "            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))\n",
    "\n",
    "            # optimize the MSE between the actual noise and the predicted noise\n",
    "            loss = mse(eta_theta, eta)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # calculates average loss over epoch\n",
    "            epoch_loss += loss.item() * len(x0) / len(dataloader.dataset)\n",
    "        # store epoch loss for plotting\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        # if sample_images then, display images generated at this epoch\n",
    "        if sample_images:\n",
    "            # sample 10 images\n",
    "            generated_images = ddpm.sample_images(n_samples=10)\n",
    "            \n",
    "            # display the generated images using the helper function\n",
    "            display_images(generated_images, labels=None, title=f\"Images generated at epoch {epoch + 1}\", num_samples=10, cols=10)\n",
    "        \n",
    "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
    "\n",
    "        # store the model if we attain  a new low loss\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(ddpm.state_dict(), store)\n",
    "            log_string += \" model stored\"\n",
    "\n",
    "        print(log_string) # print epoch loss and whether model was stored\n",
    "\n",
    "    # plotting the loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, n_epochs + 1), losses, marker='o')\n",
    "    plt.title(\"Training Loss Per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eda83c-f122-406e-9900-7ee5359c5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model\n",
    "# values defined as originally suggested by Ho et al. in DDPM paper\n",
    "T = 1000\n",
    "beta1 = 10 ** -4\n",
    "beta2 = 0.02\n",
    "schedule_type = \"linear\"\n",
    "\n",
    "# initialize our ddpm for training\n",
    "ddpm = DDPM(UNet(T), T=T, beta1=beta1, beta2=beta2, schedule_type=schedule_type, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5ca0-8cc8-4513-aef7-5251eb4713dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "# we save the best loss model at best_loss_ddpm.pt and the final 300 epoch trained model at ddpm_300.pt\n",
    "store = \"best_loss_ddpm.pt\"\n",
    "epochs = 300\n",
    "lr = 0.001\n",
    "\n",
    "training_loop(ddpm, train_loader, epochs, optim=Adam(ddpm.parameters(), lr), device=device, sample_images=True, store=store)\n",
    "torch.save(ddpm.state_dict(), 'ddpm.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86512480-8274-455e-a759-e2ea47b918f8",
   "metadata": {},
   "source": [
    "# Sampling (Optionally Loading pretrained model weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afb229-2ca2-4b59-8c71-7c0904b9a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load a pre-trained model here.\n",
    "# T = 1000\n",
    "# beta1 = 0.0001\n",
    "# beta2 = 0.02\n",
    "# schedule_type = \"linear\"\n",
    "# model = DDPM(UNet(T), T=T, beta1=beta1, beta2=beta2, schedule_type=schedule_type,  device=device)\n",
    "# model.load_state_dict(torch.load(model path))\n",
    "# model.eval()\n",
    "\n",
    "newly_generated_images = ddpm.sample_images(n_samples=100)\n",
    "display_images(newly_generated_images, labels=None, title=\"newly generated images\", num_samples=100, cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441b976-a6c5-44ef-940a-ed8adc2cc58e",
   "metadata": {},
   "source": [
    "# Quantitative Metrics Calculation (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1672c8-0cc3-4d75-a2a2-20d0b63e851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, folder=\"saved_images\"):\n",
    "    # takes a tensor of images and saves them in a specified folder\n",
    "    if not isinstance(images, torch.Tensor):\n",
    "        raise ValueError(\"Images should be a PyTorch tensor\")\n",
    "\n",
    "    images = images.detach().cpu()\n",
    "\n",
    "    # normalize and prepare images\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, img_tensor in enumerate(images):\n",
    "        img = img_tensor.squeeze()  # remove color channels\n",
    "        img_np = img.numpy()\n",
    "        plt.imsave(os.path.join(folder, f'image_{i}.png'), img_np, cmap='gray')\n",
    "\n",
    "def compute_fid(real_images_path, fake_images_path):\n",
    "    # computes the FID score between two sets of images located at the given folder paths.\n",
    "    # we set isc and kid to false as we only want to calculate FID scores\n",
    "    # we set samples_find_deep=True as we need to find images recursively within our folders\n",
    "    metrics = calculate_metrics(input1=real_images_path, input2=fake_images_path, cuda=True, isc=False, fid=True, kid=False, samples_find_deep=True)\n",
    "    return metrics['frechet_inception_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488e992-f0d0-40fc-9e6d-ac0434e8a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load a pre-trained model here.\n",
    "# T = 1000\n",
    "# beta1 = 0.0001\n",
    "# beta2 = 0.02\n",
    "# schedule_type = \"linear\"\n",
    "# model = DDPM(UNet(T), T=T, beta1=beta1, beta2=beta2, schedule_type=schedule_type, device=device)\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()\n",
    "\n",
    "# code to iteratively generate 50k images.\n",
    "# loop 5 times generating 10k each time storing in separate subfolders\n",
    "n_samples = 10000\n",
    "n_loops = 5\n",
    "folder_name = \"generated_images\"\n",
    "\n",
    "# generate (n_loops * n_samples) images\n",
    "for loop in range(n_loops):\n",
    "    # generate n_samples images using your model\n",
    "    generated_images = ddpm.sample_images(n_samples=n_samples)\n",
    "\n",
    "    # folder to save images in a loop-specific subfolder\n",
    "    loop_folder = os.path.join(\"generated_images\", f'set_{loop + 1}')\n",
    "    save_images(generated_images, loop_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca5ef4-c036-4b05-89ea-b9469b237860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load n_samples of the FashionMNIST images and save them to folder real_images\n",
    "dataset = FashionMNIST(root=\"../data\", train=True, transform=ToTensor(), download=True)\n",
    "real_images = torch.stack([dataset[i][0] for i in range(n_loops * n_samples)]) # stack real images to create a single tensor\n",
    "save_images(real_images, \"real_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb9e53-b4be-45d0-a427-24018f658284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print the FID score of synthetic vs authentic images\n",
    "fid_score = compute_fid(\"real_images\", \"generated_images\")\n",
    "print(f\"FID Score: {fid_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb77e56-d132-4a72-aa2a-4d829db4041d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
